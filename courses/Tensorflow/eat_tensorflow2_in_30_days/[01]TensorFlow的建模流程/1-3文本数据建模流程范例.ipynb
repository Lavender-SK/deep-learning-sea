{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imdb数据集的目标是根据电影评论的文本内容预测评论的情感标签。\n",
    "\n",
    "训练集有20000条电影评论文本，测试集有5000条电影评论文本，其中正面评论和负面评论都各占一半。\n",
    "\n",
    "文本数据预处理较为繁琐，包括中文切词（本示例不涉及），构建词典，编码转换，序列填充，构建数据管道等等。\n",
    "\n",
    "在tensorflow中完成文本数据预处理的常用方案有两种，\n",
    "\n",
    "- 第一种是利用 **tf.keras.preprocessing** 中的 **Tokenizer** 词典构建工具和 **tf.keras.utils.Sequence** 构建文本数据生成器管道。\n",
    "\n",
    "- 第二种是使用 **tf.data.Dataset** 搭配 **tf.keras.layers.experimental.preprocessing.TextVectorization**预处理层。\n",
    "\n",
    "第二种方法为TensorFlow原生方式，相对也更加简单一些。\n",
    "\n",
    "我们此处介绍第二种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![电影评论](../data/电影评论.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 构建管道-构建预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\" 处理每一行样本\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    line: string\n",
    "        每行样本的格式 样本标签 制表符 样本文本\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    text: tf.Tensor, dtype=string, shape=(1,)\n",
    "        样本文本\n",
    "    label: tf.Tensor, dtype=int32, shape=(1,)\n",
    "        样本标签\n",
    "    \"\"\"\n",
    "    arr = tf.strings.split(line,\"\\t\")\n",
    "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)\n",
    "    text = tf.expand_dims(arr[1],axis = 0)\n",
    "    return (text,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.strings.split: 字符串分词**\n",
    "\n",
    "- [简化NLP：TensorFlow中tf.strings的使用](https://zhuanlan.zhihu.com/p/98718689)\n",
    "\n",
    "**tf.strings.to_number:将输入张量中的每个字符串转换为指定的数值类型**\n",
    "- [TensorFlow函数：tf.string_to_number](https://www.w3cschool.cn/tensorflow_python/tensorflow_python-2kdb2my1.html)\n",
    "\n",
    "**tf.expand_dims: 扩展维度**\n",
    "- [tf.expand_dims](https://www.cnblogs.com/helloworld0604/p/9001703.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../data/imdb/train.csv\"\n",
    "test_data_path =  \"../data/imdb/test.csv\"\n",
    "\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\n",
    "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\n",
    "   .batch(BATCH_SIZE) \\\n",
    "   .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待添加注释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 构建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html,\n",
    "         '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextVectorization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bb65df6e587b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vectorize_layer = TextVectorization(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'whitespace'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_WORDS\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#有一个留给占位符\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextVectorization' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens=MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sk]",
   "language": "python",
   "name": "conda-env-sk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
